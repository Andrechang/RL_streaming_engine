Reinforcement Learning (RL) is widely used to tackle unsupervised optimization problems. It has been applied in chip placement \cite{mirhoseini2020chip}, 
workload distribution \cite{Mirhoseini_placementRNN, addanki2019placeto, zhou2019gdp}, compiler optimizations \cite{Zhou_compileGNN} and 
other decision based tasks \cite{kormushev2013reinforcement, ZophL16_NASRL}. Alternative optimization algorithms includes 
evolutionary strategies \cite{Zhichao_ESNAS} and bayesian optimization \cite{shi2020learned}. An advantage of RL approach is learning from 
a collection of programs and reuse previous data for new programs by training a Deep Learning model \cite{zhou2019gdp}.

Recurrent neural networks (RNN) \cite{hochreiter1996lstm} were popular approach to process sequence of nodes from a graph representation. 
Graph neural networks (GNN) \cite{gori2005new} showed success in processing structured data without preprocessing required for RNNs.
GNNs are widely used task involves processing graphs \cite{Zhou_compileGNN, zhou2019gdp}. In addition, attention modules are used to 
supplement the embeddings created by GNNs to further improve results \cite{addanki2019placeto}.

In \cite{mirhoseini2020chip}, a RL method for chip placement is presented. A graph neural network to create embedding from a netlist graph and passed through an actor model via PPO. 
The produced output is the whole chip placement. Their actor is composed of deconvolution layers which are more computationally intensive. 
\cite{zhou2019gdp} presents a combination of graph neural network and transformer-XL model to place operations on devices. 

In our case, the input is a combination of computation graph, SE device state and node to be placed. 
Instead of only feeding our actor model with embedding from graph neural network, we combine the graph neural network embedding with information from the SE device and a representation of node that is going to be placed. 
A configuration is also added to guide the model to optimize different goals.  

Another difference from \cite{zhou2019gdp}  is that our strategy is to place one computation node at a time, instead of generating a whole assignment per iteration. 
This allows the model to break down the placement problem into sub-problems. 
This also allows the framework to start from a different start point. For example, if some nodes are already placed by some other algorithm, the RL mapper can place the remaining nodes. 
This approach also allows us to sample more data during sampling phase. 
It can save per node placement, instead of only saving a sample for an entire sequence.  